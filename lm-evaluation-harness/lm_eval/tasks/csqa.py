import numpy as np
from lm_eval.base import rf, Task
from lm_eval.metrics import mean
from lm_eval.utils import InstructionTemplates
import re

class CSQA(Task):
    VERSION = 0
    DATASET_PATH = "mbzuai-ugrip-statement-tuning/X-CSQA"
    DATASET_NAME = "en"

    # ORCA_SYSTEM = (
    #     "While answering a multiple choice question, first output the correct answer(s). "
    # )

    ORCA_SYSTEM = (
        "You should describe the task and explain your answer. "
        "While answering a multiple choice question, first output the correct answer(s). "
        "Then explain why other answers are wrong. Think like you are answering to a five year old."
    )
    _INSTRUCTIONS_CSQA = \
"""Read the following commonsense question carefully, then choose the best answer from the given options. The answer should be one of A, B, C, D, E.

<Question>:
{question}

<Choices>:
A. {choice_A}
B. {choice_B}
C. {choice_C}
D. {choice_D}
E. {choice_E}

<Answer>:"""

    def has_training_docs(self):
        return False

    def has_validation_docs(self):
        return False

    def has_test_docs(self):
        return True

    def test_docs(self):
        return self.dataset["test"]

    def doc_to_text(self, doc, instruction_template=None):
        if instruction_template:
            return self.doc_to_text_with_instruction(doc, instruction_template)
        else:
            raise NotImplementedError

    def doc_to_text_with_instruction(self, doc, instruction_template="base"):
        instruction = self._INSTRUCTIONS_CSQA.format(
            question=doc["question"],
            choice_A=doc["choice_A"],
            choice_B=doc["choice_B"],
            choice_C=doc["choice_C"],
            choice_D=doc["choice_D"],
            choice_E=doc["choice_E"],
        )
        if instruction_template == "orca":
            template = InstructionTemplates.get_template(instruction_template)
            return template.format(
                system_message=self.ORCA_SYSTEM,
                user_message=instruction,
            )
        elif instruction_template == "base":
            return instruction
        else:
            raise ValueError(
                f"Unknown instruction template: {instruction_template}")

    def doc_to_target(self, doc, instruction_template="base"):
        if instruction_template:
            return self.doc_to_target_with_instruction(doc, instruction_template)
        else:
            raise NotImplementedError

    def doc_to_target_with_instruction(self, doc, instruction_template="base"):
        correct_choice = f'Choice {doc["answerKey"]}.'
        return correct_choice

    # def construct_requests(self, doc, ctx, instruction_template=None):
    #     if instruction_template:
    #         choice1 = 'Choice A.'
    #         choice2 = 'Choice B.'
    #         choice3 = 'Choice C.'
    #         choice4 = 'Choice D.'
    #         choice5 = 'Choice E.'
    #     else:
    #         raise NotImplementedError

    #     ll_choice1, _ = rf.loglikelihood(ctx, choice1)
    #     ll_choice2, _ = rf.loglikelihood(ctx, choice2)
    #     ll_choice3, _ = rf.loglikelihood(ctx, choice3)
    #     ll_choice4, _ = rf.loglikelihood(ctx, choice4)
    #     ll_choice5, _ = rf.loglikelihood(ctx, choice5)

    #     return ll_choice1, ll_choice2, ll_choice3, ll_choice4, ll_choice5

    def construct_requests(self, doc, ctx, instruction_template=None):
        """Uses RequestFactory to construct Requests and returns an iterable of
        Requests which will be sent to the LM.

        :param doc:
            The document as returned from training_docs, validation_docs, or test_docs.
        :param ctx: str
            The context string, generated by fewshot_context. This includes the natural
            language description, as well as the few shot examples, and the question
            part of the document for `doc`.
        """
        return rf.greedy_until(ctx, {"until": ['</s>', '\n']})

    # def process_results(self, doc, results):
    #     gold = ord(doc["answerKey"]) - ord('A')
    #     pred = np.argmax(results)
    #     acc = 1.0 if pred == gold else 0.0
    #     return {"acc": acc}

    def process_results(self, doc, results):
        """Take a single document and the LM results and evaluates, returning a
        dict where keys are the names of submetrics and values are the values of
        the metric for that one document

        :param doc:
            The document as returned from training_docs, validation_docs, or test_docs.
        :param results:
            The results of the requests created in construct_requests.
        """
        gold = doc['answerKey']
        completion = results[0]
        pred = re.search(r'\b([A-Za-z])\b\.', completion)
        if pred is not None:
            pred = pred[0][0]
            acc = 1.0 if gold == pred else 0.0
        else:
            acc = 0.0
        return {"acc": acc}

    def higher_is_better(self):
        return {"acc": True}

    def aggregation(self):
        return {"acc": mean}



class CSQA_ar(CSQA):  # AR
    DATASET_NAME = "ar"

class CSQA_de(CSQA):  # DE
    DATASET_NAME = "de"

class CSQA_en(CSQA):  # EN
    DATASET_NAME = "en"

class CSQA_es(CSQA):  # ES
    DATASET_NAME = "es"

class CSQA_fr(CSQA):  # FR
    DATASET_NAME = "fr"

class CSQA_hi(CSQA):  # HI
    DATASET_NAME = "hi"

class CSQA_it(CSQA):  # IT
    DATASET_NAME = "it"

class CSQA_ja(CSQA):  # JA
    DATASET_NAME = "ja"

class CSQA_nl(CSQA):  # NL
    DATASET_NAME = "nl"

class CSQA_pl(CSQA):  # PL
    DATASET_NAME = "pl"

class CSQA_pt(CSQA):  # PT
    DATASET_NAME = "pt"

class CSQA_ru(CSQA):  # RU
    DATASET_NAME = "ru"

class CSQA_sw(CSQA):  # SW
    DATASET_NAME = "sw"

class CSQA_ur(CSQA):  # UR
    DATASET_NAME = "ur"

class CSQA_vi(CSQA):  # VI
    DATASET_NAME = "vi"

class CSQA_zh(CSQA):  # ZH
    DATASET_NAME = "zh"

LANG_CLASSES = [
    CSQA_ar, CSQA_de, CSQA_en, CSQA_es, CSQA_fr, CSQA_hi, CSQA_it, CSQA_ja, 
    CSQA_nl, CSQA_pl, CSQA_pt, CSQA_ru, CSQA_sw, CSQA_ur, CSQA_vi, CSQA_zh,
]


LANGS = """
ar  de  en  es  fr  hi  it  ja  nl  pl  pt  ru  sw  ur  vi  zh
""".split()


def construct_tasks():
    tasks = {}
    for lang, lang_class in zip(LANGS, LANG_CLASSES):
        tasks[f"csqa_{lang}"] = lang_class
    return tasks
